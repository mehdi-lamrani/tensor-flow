{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aQKQMJTJBPH"
      },
      "source": [
        "# Q* Learning with FrozenLake 4x4 \n",
        "\n",
        "In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
        "\n",
        "![alt text](http://simoninithomas.com/drlc/Qlearning/frozenlake4x4.png)\n",
        "\n",
        "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, **so you won't always move in the direction you intend (stochastic environment)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK8fD0zAQkkg"
      },
      "source": [
        "## Prerequisites 🏗️\n",
        "Before diving on the notebook **you need to understand**:\n",
        "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
        "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
        "- In the [video version](https://www.youtube.com/watch?v=q2ZOEFAaaI0)  we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 🚕 with Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54AIvDov_7aa"
      },
      "source": [
        "## Step -1: Install the dependencies on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxxpHDIs_lvg",
        "outputId": "d94411eb-3a2b-48f1-ef3a-a8ed0cf28ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.7.4. in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.7.4.) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.7.4.) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.7.4.) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.7.4.) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.7.4.) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.7.4.) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.7.4.) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.7.4.) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install gym==0.7.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oU8zRXv8QHlm"
      },
      "outputs": [],
      "source": [
        "#import the required libraries.\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh9jBR_cQ5_a",
        "outputId": "b99984e8-a07b-431d-fee2-f9eaaa07d476"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake-v0\n",
            "[2022-11-03 23:12:46,187] Making new env: FrozenLake-v0\n",
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        }
      ],
      "source": [
        "#create the environment usign OpenAI Gym\n",
        "env = gym.make(\"FrozenLake-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtXMldxQ7uw"
      },
      "source": [
        "## Step 2: Create the Q-table and initialize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc0xDVd_Q-C8",
        "outputId": "887083de-66e0-4d3c-a1fd-a309b3334ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space : 4 | State Space: 16\n"
          ]
        }
      ],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "print(f\"Action Space : {action_size} | State Space: {state_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J_GfR-p25bq",
        "outputId": "e19e73a3-853f-417f-942a-29ebdf369bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16, 4)\n"
          ]
        }
      ],
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DbAR9J_3DXa"
      },
      "source": [
        "## Step 3: Create Required Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dBHB8MIl71Aw"
      },
      "outputs": [],
      "source": [
        "total_episodes = 15000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqu-5j9B7qmy"
      },
      "source": [
        "## Step 4 : Q-Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJYnA88a3TmG",
        "outputId": "e1c7691e-ba9c-4550-d57a-da2a8a1a6d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score over time: 0.49273333333333336\n",
            "[[2.20407360e-01 8.96880726e-02 5.65018811e-02 4.37047662e-02]\n",
            " [6.63294149e-03 3.90186520e-03 7.64110663e-04 2.90374276e-01]\n",
            " [1.60212929e-02 5.98514045e-03 1.22272021e-02 3.62965772e-02]\n",
            " [1.16813992e-04 7.36185907e-04 9.71641081e-03 1.97779170e-02]\n",
            " [2.22674784e-01 5.51645868e-02 5.25369395e-02 4.64040778e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.74216727e-07 2.64618471e-10 4.17136357e-02 5.66774122e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.15684136e-02 1.88407323e-02 2.32635044e-03 2.24850258e-01]\n",
            " [2.09736279e-02 2.75917466e-01 2.36420978e-03 1.04453212e-01]\n",
            " [3.36028054e-01 1.43875589e-04 4.00432660e-02 3.61765704e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.75982685e-02 2.81086205e-02 6.97131392e-01 4.39025189e-02]\n",
            " [2.76346293e-01 9.44984317e-01 1.03063754e-01 2.06995204e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# List of rewards\n",
        "rewards = #fill here\n",
        "\n",
        "#until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(#fill here):\n",
        "        #Choose an action a in the current world state (s)\n",
        "        exp_exp_tradeoff = random.#fill here(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(#fill here[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(#fill here)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + #fill here\n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(#fill here) \n",
        "\n",
        "    rewards.append(#fill here)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQvoFSsr3TkM",
        "outputId": "852a77de-67ea-458d-ce89-ea0f98055ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 6\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 76\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 43\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 12\n",
            "****************************************************\n",
            "EPISODE  4\n"
          ]
        }
      ],
      "source": [
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", #fill here)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(#fill here)\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", #fill here)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Q Learning with OpenAI Gym - FrozenLake.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "d8e65be2d5af5c08928ecb773b6dc450231fb3ffc1135c3e29100eb7fa86bbe1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
